r'''
UniRef
======

.. currentmodule:: micronota.db.uniref

This module create databases from UniRef for micronota usage. Using UniRef
for functional assignment of coding genes, we can get the clustering of
the annotated proteins for free. Using UniRef100 as the reference database,
we can easily collapse the clusters down to the similarity levels
of 90% or 50%. For those UniRef records from UniProKB, we also transfer
the metadata associated with those UniProKB records.

UniProtKB (UniProt Knowledgebase) [#]_ contains two parts, UniProtKB/Swiss-Prot
and UniProtKB/TrEMBL. The first part contains protein records that are
manually annotated and reviewed while the second part is done computationally
and not manually reviewed.

UniParc (UniProt Archive) [#]_ is also part of UniProt project. It is a
is a comprehensive and non-redundant database that contains most of the
publicly available protein sequences in the world.

The UniProt Reference Clusters (UniRef) [#]_ provide clustered sets (UniRef100,
UniRef90 and UniRef50 clusters) of sequences from the UniProtKB
and selected UniParc records, in order to obtain complete coverage of sequence
space at several resolutions (100%, >90% and >50%) while hiding redundant
sequences (but not their descriptions) from view.

If you used this database through micronota, you should also `cite UniProt
<http://www.uniprot.org/help/publications>`_ properly.

Release
-------
:version: 2016_01
:date:    01/20/2016
:link:    ftp://ftp.uniprot.org/pub/databases/uniprot/relnotes.txt

Files
-----
* UniRef files

  UniRef fasta files clustered at the similarity levels of 50, 90 or 100.
  The UniRef100 identifier is generated by placing ``UniRef100_`` prefix
  before the UniProtKB accession number or UniParc identifier of the
  representative UniProtKB or UniParc entry. It is similarly done for
  UniRef50 or UniRef90.

  * uniref50.fasta.gz [#]_

  * uniref90.fasta.gz [#]_

  * uniref100.fasta.gz [#]_

* UniProtKB files

  The .dat.gz files contain the UniProtKB records in a variant format of EMBL.
  The records are divided into taxa groups [#]_.

  * uniprot_sprot.dat.gz

  * uniprot_trembl.dat.gz

Reference
---------
.. [#] http://www.uniprot.org/help/uniparc
.. [#] http://www.uniprot.org/help/uniprotkb
.. [#] http://www.ncbi.nlm.nih.gov/pubmed/17379688
.. [#] ftp://ftp.uniprot.org/pub/databases/uniprot/uniref/uniref50/README
.. [#] ftp://ftp.uniprot.org/pub/databases/uniprot/uniref/uniref90/README
.. [#] ftp://ftp.uniprot.org/pub/databases/uniprot/uniref/uniref100/README
.. [#] ftp://ftp.uniprot.org/pub/databases/uniprot/current_release/knowledgebase/taxonomic_divisions/README
'''

# ----------------------------------------------------------------------------
# Copyright (c) 2015--, micronota development team.
#
# Distributed under the terms of the Modified BSD License.
#
# The full license is in the file COPYING.txt, distributed with this software.
# ----------------------------------------------------------------------------

from os.path import join, basename
from sqlite3 import connect
from xml.etree import ElementTree as ET
from tempfile import mkdtemp
import shutil
import gzip

from skbio import read, Sequence

from ..util import _overwrite_file, download


def prepare_db(out_d, prefix='uniref100', force=False,
               sprot='ftp://ftp.uniprot.org/pub/databases/uniprot/current_release/knowledgebase/complete/uniprot_sprot.xml.gz',
               trembl='ftp://ftp.uniprot.org/pub/databases/uniprot/current_release/knowledgebase/complete/uniprot_trembl.xml.gz',
               uniref100='ftp://ftp.uniprot.org/pub/databases/uniprot/uniref/uniref100/uniref100.fasta.gz',
               id_map='ftp://ftp.uniprot.org/pub/databases/uniprot/current_release/knowledgebase/idmapping/idmapping_selected.tab.gz'):
    '''Prepare reference database for UniRef.

    Notes
    -----
    This function creates the following files:

    * ``uniref100_sprot_archaea.dmnd``
    * ``uniref100_sprot_bacteria.dmnd``
    * ``uniref100_sprot_viruses.dmnd``
    * ``uniref100_sprot_other.dmnd``
    * ``uniref100_trembl_archaea.dmnd``
    * ``uniref100_trembl_bacteria.dmnd``
    * ``uniref100_trembl_viruses.dmnd``
    * ``uniref100_trembl_other.dmnd``
    * ``uniref100__other.dmnd``

    * ``uniprotkb.db``
    * ``uniprotkb_id_map.db``: the map of IDs between databases
    '''
    try:
        temp_dir = mkdtemp()

        id_map_fp = join(out_d, 'uniprotkb_id_map.db')
        id_map_tmp = join(temp_dir, basename(id_map))
        download(id_map, id_map_tmp)
        create_id_map(id_map_tmp, id_map_fp)

        metadata_fp = join(out_d, 'uniprotkb.db')
        sprot_tmp = join(temp_dir, basename(sprot))
        download(sprot, sprot_tmp)
        prepare_metadata(sprot_tmp, metadata_fp)
        trembl_tmp = join(temp_dir, basename(trembl))
        download(trembl, trembl_tmp)
        prepare_metadata(sprot_tmp, metadata_fp)

        uniref100_tmp = join(temp_dir, basename(uniref100))
        download(uniref100, uniref100_tmp)
        sort_uniref(uniref100_tmp, out_d)
    finally:
        shutil.rmtree(temp_dir)
    # prepare_metadata(join(expanduser('~'), 'uniref', 'uniprot_sprot.xml.gz'),
    #                  '%s.db' % join(out_d, prefix))
    # prepare_metadata(join(expanduser('~'), 'uniref', 'uniprot_trembl.xml.gz'),
    #                  '%s.db' % join(out_d, prefix))
    # prepare_metadata(join(expanduser('~'), 'uniref', 'uniprot_trembl.dat.gz'),
    #                  '%s.db' % join(out_d, prefix))
    # sort_uniref('%s.db' % join(out_d, prefix), join(expanduser('~'), 'uniref', 'uniref100.fasta.gz'), out_d)
    # create_id_map(join(expanduser('~'), 'compy', 'uniref', 'idmapping_selected.tab'),
    #               'id_map.db')


def create_id_map(in_fp, db_fp, overwrite=True):
    '''Map between UniRef100, UniRef90, UniRef50, UniProtKB and other databases.

    Notes
    -----
    1. UniProtKB_AC
    2. UniProtKB_ID
    3. GeneID (EntrezGene)
    4. RefSeq
    5. GI
    6. PDB
    7. GO
    8. UniRef100
    9. UniRef90
    10. UniRef50
    11. UniParc
    12. PIR
    13. NCBI_taxon
    14. MIM
    15. UniGene
    16. PubMed
    17. EMBL
    18. EMBL_CDS
    19. Ensembl
    20. Ensembl_TRS
    21. Ensembl_PRO
    22. Additional_PubMed
    '''
    _overwrite_file(db_fp, overwrite)
    with connect(db_fp) as conn:
        table_name = 'id_map'
        conn.execute('''CREATE TABLE IF NOT EXISTS {t} (
                          UniProtKB_AC TEXT,
                          EntrezGene TEXT,
                          RefSeq TEXT,
                          GI TEXT,
                          PDB TEXT,
                          GO TEXT,
                          UniRef100 TEXT,
                          UniRef90 TEXT,
                          UniRef50 TEXT,
                          UniParc TEXT,
                          PIR TEXT,
                          NCBI_taxon TEXT,
                          MIM TEXT,
                          UniGene TEXT,
                          PubMed TEXT,
                          EMBL TEXT,
                          EMBL_CDS TEXT,
                          Ensembl TEXT,
                          Ensembl_TRS TEXT,
                          Ensembl_PRO TEXT,
                          Additional_PubMed TEXT)'''.format(t=table_name))

        # cur.execute('BEGIN TRANSACTION')
        for n, line in enumerate(gzip.open(in_fp), 1):
            items = line.decode('utf8').split('\t')
            del items[1]
            insert = 'INSERT INTO {t} VALUES ({p});'.format(
                t=table_name, p=', '.join(['?'] * 21))

            conn.execute(insert, items)

        # don't forget to index the column to speed up query
        conn.execute('CREATE INDEX IF NOT EXISTS UniRef100 ON {t} (UniRef100)'.format(
            t=table_name))
        conn.commit()

    return n


def sort_uniref(db_fp, uniref_fp, out_d):
    '''Sort UniRef sequences into different partitions.'''
    name_map = {'Swiss-Prot': 'sprot',
                'TrEMBL': 'trembl'}
    fnames = ['%s_%s' % (i, j) for i in ['sprot', 'trembl']
              for j in ['Bacteria', 'Archaea', 'Viruses', 'other']]
    fnames.append('_other')
    files = {f: open(join(out_d, 'uniref100_%s.fasta' % f), 'w')
             for f in fnames}

    with connect(db_fp) as conn:
        cursor = conn.cursor()
        for seq in read(uniref_fp, format='fasta', constructor=Sequence):
            id = seq.metadata['id']
            ac = id.replace('UniRef100_', '')
            group = ['', 'other']
            cursor.execute('''SELECT * FROM metadata
                              WHERE ac = ?''',
                           (ac,))
            for _, dataset, taxon, _ in cursor.fetchall():
                group[0] = name_map.get(dataset, '')
                if taxon in ['Bacterial', 'Archaea', 'Viruses']:
                    group[1] = taxon
            seq.write(files['_'.join(group)])

    for f in files:
        files[f].close()


def prepare_metadata(in_fp, out_fp, overwrite=False):
    '''
    Returns
    -------
    int
        The number of records processed.

    Notes
    -----
    The schema of the database file contains one table named `tigrfam` that
    has following columns:

    1. ``ac``. TEXT. UniProtKB accession.

    2. ``dataset``. TEXT.

    3. ``taxon``. TEXT.

    4. ``transfer``. INTEGER. Used as the boolean. ``1`` means the ``val``
       should be transferred to the query sequences as its annotation;
       ``0`` means not.

    The table in the database file will be dropped and re-created if
    the function is re-run.
    '''
    _overwrite_file(out_fp, overwrite)
    with connect(out_fp) as conn:
        table_name = 'metadata'
        conn.execute('''CREATE TABLE IF NOT EXISTS {t} (
                            ac       TEXT    NOT NULL,
                            key      TEXT    NOT NULL,
                            val      BLOB    NOT NULL,
                            transfer BOOLEAN NOT NULL,
                        CHECK (transfer IN (0,1)));'''.format(t=table_name))

        for n, parsed in enumerate(_parse_xml(in_fp), 1):
            insert = '''INSERT INTO {t} (ac, key, val, transfer)
                        VALUES (?,?,?,?);'''.format(t=table_name)
            conn.execute(insert, (parsed[0], parsed[1], parsed[2], 0))
        # don't forget to index the column to speed up query
        conn.execute('CREATE INDEX IF NOT EXISTS ac ON {t} (ac);'.format(
            t=table_name))
        conn.commit()

    return n


def _parse_xml(in_fp):
    def fixtag(ns, tag, nsmap):
        return '{%s}%s' % (nsmap[ns], tag)

    ns_map = {'xmlns': 'http://uniprot.org/uniprot',
              'xsi': 'http://www.w3.org/2001/XMLSchema-instance'}
    # it is very important to set the events to 'end'
    for event, elem in ET.iterparse(gzip.open(in_fp), events=['end']):
        if elem.tag == fixtag('xmlns', 'entry', ns_map):
            yield _process_entry(elem, ns_map)
            # this is necessary for garbage collection
            elem.clear()


def _process_entry(root, ns_map):
    group = root.attrib['dataset']
    try:
        accession = root.find('./xmlns:accession', ns_map).text
    except AttributeError:
        for child in root:
            print(child.tag, child.text)
    try:
        taxon = root.find('./xmlns:organism/xmlns:lineage/xmlns:taxon',
                          ns_map).text
    except AttributeError:
        taxon = None

    return accession, group, taxon
