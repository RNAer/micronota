r'''
UniRef
======

.. currentmodule:: micronota.db.uniref

This module create databases from UniRef for micronota usage. Using UniRef
for functional assignment of coding genes, we can get the clustering of
the annotated proteins for free. Using UniRef100 as the reference database,
we can easily collapse the clusters down to the similarity levels
of 90% or 50%. For those UniRef records from UniProKB, we also transfer
the metadata associated with those UniProKB records.

UniProtKB (UniProt Knowledgebase) [#]_ contains two parts, UniProtKB/Swiss-Prot
and UniProtKB/TrEMBL. The first part contains protein records that are
manually annotated and reviewed while the second part is done computationally
and not manually reviewed.

UniParc (UniProt Archive) [#]_ is also part of UniProt project. It is a
is a comprehensive and non-redundant database that contains most of the
publicly available protein sequences in the world.

The UniProt Reference Clusters (UniRef) [#]_ provide clustered sets (UniRef100,
UniRef90 and UniRef50 clusters) of sequences from the UniProtKB
and selected UniParc records, in order to obtain complete coverage of sequence
space at several resolutions (100%, >90% and >50%) while hiding redundant
sequences (but not their descriptions) from view.

If you used this database through micronota, you should also `cite UniProt
<http://www.uniprot.org/help/publications>`_ properly.

Release
-------
:version: 2016_01
:date:    01/20/2016
:link:    ftp://ftp.uniprot.org/pub/databases/uniprot/relnotes.txt

Files
-----
* UniRef files

  UniRef fasta files clustered at the similarity levels of 50, 90 or 100.
  The UniRef100 identifier is generated by placing ``UniRef100_`` prefix
  before the UniProtKB accession number or UniParc identifier of the
  representative UniProtKB or UniParc entry. It is similarly done for
  UniRef50 or UniRef90.

  * uniref50.fasta.gz [#]_

  * uniref90.fasta.gz [#]_

  * uniref100.fasta.gz [#]_

* UniProtKB files

  The .dat.gz files contain the UniProtKB records in a variant format of EMBL.
  The records are divided into taxa groups [#]_.

  * uniprot_sprot.dat.gz

  * uniprot_trembl.dat.gz

Reference
---------
.. [#] http://www.uniprot.org/help/uniparc
.. [#] http://www.uniprot.org/help/uniprotkb
.. [#] http://www.ncbi.nlm.nih.gov/pubmed/17379688
.. [#] ftp://ftp.uniprot.org/pub/databases/uniprot/uniref/uniref50/README
.. [#] ftp://ftp.uniprot.org/pub/databases/uniprot/uniref/uniref90/README
.. [#] ftp://ftp.uniprot.org/pub/databases/uniprot/uniref/uniref100/README
.. [#] ftp://ftp.uniprot.org/pub/databases/uniprot/current_release/knowledgebase/taxonomic_divisions/README
'''

# ----------------------------------------------------------------------------
# Copyright (c) 2015--, micronota development team.
#
# Distributed under the terms of the Modified BSD License.
#
# The full license is in the file COPYING.txt, distributed with this software.
# ----------------------------------------------------------------------------

from os.path import join, basename
from sqlite3 import connect
from xml.etree import ElementTree as ET
import gzip

from skbio import read, Sequence

from ..util import _overwrite, _download


def prepare_db(out_d, downloaded, force=False,
               sprot='ftp://ftp.uniprot.org/pub/databases/uniprot/current_release/knowledgebase/complete/uniprot_sprot.xml.gz',
               trembl='ftp://ftp.uniprot.org/pub/databases/uniprot/current_release/knowledgebase/complete/uniprot_trembl.xml.gz',
               uniref100='ftp://ftp.uniprot.org/pub/databases/uniprot/uniref/uniref100/uniref100.fasta.gz',
               id_map='ftp://ftp.uniprot.org/pub/databases/uniprot/current_release/knowledgebase/idmapping/idmapping_selected.tab.gz'):
    '''Prepare reference database for UniRef.

    Parameters
    ----------
    out_d : str
        The output directory
    force : boolean
        Force overwrite the files
    downloaded : str
        File directory. If the files are already downloaded,
        just use the files in the directory and skip _download.

    Notes
    -----
    This function creates the following files:

    * ``uniref100_sprot_Archaea.dmnd``
    * ``uniref100_sprot_Bacteria.dmnd``
    * ``uniref100_sprot_Viruses.dmnd``
    * ``uniref100_sprot_other.dmnd``
    * ``uniref100_trembl_Archaea.dmnd``
    * ``uniref100_trembl_Bacteria.dmnd``
    * ``uniref100_trembl_Viruses.dmnd``
    * ``uniref100_trembl_other.dmnd``
    * ``uniref100__other.dmnd``

    * ``uniprotkb.db``
    * ``uniprotkb_id_map.db``: the map of IDs between databases
    '''

    id_map_fp = join(out_d, 'uniprotkb_id_map.db')
    id_map_raw = join(downloaded, basename(id_map))
    metadata_fp = join(out_d, 'uniprotkb.db')
    sprot_raw = join(downloaded, basename(sprot))
    trembl_raw = join(downloaded, basename(trembl))
    uniref100_raw = join(downloaded, basename(uniref100))
    try:
        _download(id_map, id_map_raw, overwrite=force)
        _download(sprot, sprot_raw, overwrite=force)
        _download(trembl, trembl_raw, overwrite=force)
        _download(uniref100, uniref100_raw, overwrite=force)
    except FileExistsError:
        pass

    create_id_map(id_map_raw, id_map_fp, force)

    prepare_metadata(sprot_raw, metadata_fp, append=False)
    prepare_metadata(trembl_raw, metadata_fp, append=True)

    sort_uniref(uniref100_raw, out_d, force)


def create_id_map(in_fp, db_fp, overwrite=False):
    '''Map between UniRef100, UniRef90, UniRef50, UniProtKB and other databases.

    Parameters
    ----------
    in_fp : str
        The gzipped input id map file downloaded from UniProt ftp.
    db_fp : str
        The output database file.
    overwrite : boolean
        Whether to overwrite the ``db_fp`` if it is already exist.

    Returns
    -------
    int
        The number of records processed.

    Notes
    -----
    The input ``in_fp`` should have following columns in order:

    1. UniProtKB_AC
    2. UniProtKB_ID
    3. GeneID (EntrezGene)
    4. RefSeq
    5. GI
    6. PDB
    7. GO
    8. UniRef100
    9. UniRef90
    10. UniRef50
    11. UniParc
    12. PIR
    13. NCBI_taxon
    14. MIM
    15. UniGene
    16. PubMed
    17. EMBL
    18. EMBL_CDS
    19. Ensembl
    20. Ensembl_TRS
    21. Ensembl_PRO
    22. Additional_PubMed

    All will be consumed to ``db_fp`` except 2nd column.

    '''
    _overwrite(db_fp, overwrite)
    with connect(db_fp) as conn:
        table_name = 'id_map'
        conn.execute('''CREATE TABLE IF NOT EXISTS {t} (
                          UniProtKB_AC TEXT,
                          EntrezGene TEXT,
                          RefSeq TEXT,
                          GI TEXT,
                          PDB TEXT,
                          GO TEXT,
                          UniRef100 TEXT,
                          UniRef90 TEXT,
                          UniRef50 TEXT,
                          UniParc TEXT,
                          PIR TEXT,
                          NCBI_taxon TEXT,
                          MIM TEXT,
                          UniGene TEXT,
                          PubMed TEXT,
                          EMBL TEXT,
                          EMBL_CDS TEXT,
                          Ensembl TEXT,
                          Ensembl_TRS TEXT,
                          Ensembl_PRO TEXT,
                          Additional_PubMed TEXT)'''.format(t=table_name))

        # cur.execute('BEGIN TRANSACTION')
        for n, line in enumerate(gzip.open(in_fp), 1):
            items = line.decode('utf8').split('\t')
            del items[1]
            insert = 'INSERT INTO {t} VALUES ({p});'.format(
                t=table_name, p=', '.join(['?'] * 21))

            conn.execute(insert, items)

        # don't forget to index the column to speed up query
        conn.execute('''CREATE INDEX IF NOT EXISTS
                          UniRef100 ON {t} (UniRef100)'''.format(
                              t=table_name))
        conn.commit()

    return n


def sort_uniref(db_fp, uniref_fp, out_d, overwrite=False):
    '''Sort UniRef sequences into different partitions.

    This will sort UniRef100 seq into following partitions based on both
    quality and taxon:

    * ``uniref100_sprot_Archaea.fasta``
    * ``uniref100_sprot_Bacteria.fasta``
    * ``uniref100_sprot_Viruses.fasta``
    * ``uniref100_sprot_other.fasta``
    * ``uniref100_trembl_Archaea.fasta``
    * ``uniref100_trembl_Bacteria.fasta``
    * ``uniref100_trembl_Viruses.fasta``
    * ``uniref100_trembl_other.fasta``
    * ``uniref100__other.fasta``

    Parameters
    ----------
    db_fp : str
        The database file created by ``prepare_metadata``.
    uniref_fp : str
        The UniRef100 fasta file. gzipped or not.
    out_d : str
        The output directory.
    '''
    name_map = {'Swiss-Prot': 'sprot',
                'TrEMBL': 'trembl'}
    fns = ['%s_%s' % (i, j) for i in ['sprot', 'trembl']
           for j in ['Bacteria', 'Archaea', 'Viruses', 'other']]
    fns.append('_other')
    fps = [join(out_d, 'uniref100_%s.fasta') % f for f in fns]

    for f in fns:
        _overwrite(f, overwrite)

    files = {fn: open(fp, 'w') for fp, fn in zip(fps, fns)}

    with connect(db_fp) as conn:
        cursor = conn.cursor()
        for seq in read(uniref_fp, format='fasta', constructor=Sequence):
            id = seq.metadata['id']
            ac = id.replace('UniRef100_', '')
            group = ['', 'other']
            cursor.execute('''SELECT * FROM metadata
                              WHERE ac = ?''',
                           (ac,))
            for _, dataset, taxon, _ in cursor.fetchall():
                group[0] = name_map.get(dataset, '')
                if taxon in ['Bacterial', 'Archaea', 'Viruses']:
                    group[1] = taxon
            seq.write(files['_'.join(group)])

    for f in files:
        files[f].close()


def prepare_metadata(in_fp, out_fp, append=True):
    '''
    Parameters
    ----------
    in_fp : str
        The gzipped xml file of either UniProtKB Swiss-Prot or TrEMBLE.
    out_fp : str
        The output database file. See ``Notes``.
    append : boolean
        Whether to append to the current ``out_fp`` if it exists.

    Returns
    -------
    int
        The number of records processed.

    Notes
    -----
    The schema of the database file contains one table named `tigrfam` that
    has following columns:

    1. ``ac``. TEXT. UniProtKB accession.

    2. ``dataset``. TEXT.

    3. ``taxon``. TEXT.

    4. ``transfer``. INTEGER. Used as the boolean. ``1`` means the ``val``
       should be transferred to the query sequences as its annotation;
       ``0`` means not.

    The table in the database file will be dropped and re-created if
    the function is re-run.
    '''
    _overwrite(out_fp, append=append)
    with connect(out_fp) as conn:
        table_name = 'metadata'
        conn.execute('''CREATE TABLE IF NOT EXISTS {t} (
                            ac       TEXT    NOT NULL,
                            key      TEXT    NOT NULL,
                            val      BLOB    NOT NULL,
                            transfer BOOLEAN NOT NULL,
                        CHECK (transfer IN (0,1)));'''.format(t=table_name))

        for n, parsed in enumerate(_parse_xml(in_fp), 1):
            insert = '''INSERT INTO {t} (ac, key, val, transfer)
                        VALUES (?,?,?,?);'''.format(t=table_name)
            conn.execute(insert, (parsed[0], parsed[1], parsed[2], 0))
        # don't forget to index the column to speed up query
        conn.execute('CREATE INDEX IF NOT EXISTS ac ON {t} (ac);'.format(
            t=table_name))
        conn.commit()

    return n


def _parse_xml(in_fp):
    def fixtag(ns, tag, nsmap):
        return '{%s}%s' % (nsmap[ns], tag)
    # this is the namespace for uniprot xml files.
    ns_map = {'xmlns': 'http://uniprot.org/uniprot',
              'xsi': 'http://www.w3.org/2001/XMLSchema-instance'}
    # it is very important to set the events to 'end'; otherwise,
    # elem would be an incomplete record.
    for event, elem in ET.iterparse(gzip.open(in_fp), events=['end']):
        if elem.tag == fixtag('xmlns', 'entry', ns_map):
            yield _process_entry(elem, ns_map)
            # this is necessary for garbage collection
            elem.clear()


def _process_entry(root, ns_map):
    group = root.attrib['dataset']
    try:
        accession = root.find('./xmlns:accession', ns_map).text
    except AttributeError:
        for child in root:
            print(child.tag, child.text)
    try:
        taxon = root.find('./xmlns:organism/xmlns:lineage/xmlns:taxon',
                          ns_map).text
    except AttributeError:
        taxon = None

    return accession, group, taxon
